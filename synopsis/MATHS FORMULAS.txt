5.2. Module 1: Virtual Try-On Engine (CatVTON)
•	Pipeline:
1.	Input: The system receives a person's image (IP) and a garment image (IC).
2.	Preprocessing: We perform pose estimation using MediaPipe to get body keypoints and human parsing using U²Net to segment the person. This creates an "agnostic" representation of the person (IA) with their original clothes masked out.
3.	Cloth Warping: A Cross-Attention Transformer learns a dense flow field to warp the garment (IC) to align perfectly with the person's pose and body shape, creating the warped cloth (IW).
4.	Image Synthesis: A GAN generator, conditioned on the agnostic person and the warped cloth, produces the final, photorealistic try-on image (Ifinal).
•	Mathematical Formulation: The model is trained adversarially.
o	GAN Objective: The core is the minimax game between the Generator (G) and Discriminator (D), where G tries to fool D. $$ \min_G \max_D V(D, G) = \mathbb{E}{x \sim p{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))] $$
o	Total Loss: To ensure high quality, the GAN loss is combined with other losses, such as a perceptual loss (for realism) and a warp loss (for alignment accuracy). $$ L_{total} = L_{GAN} + \lambda_1 L_{perceptual} + \lambda_2 L_{warp} $$
5.3. Module 2: Fashion Recommendation System
•	Collaborative Filtering: This component models the collective behavior of users. We will use Matrix Factorization, where the user-item interaction matrix (R) is decomposed into low-dimensional user (U) and item (V) latent factor matrices. $$ R \approx U \cdot V^T $$
•	Content-Based Filtering: This component recommends items based on their visual features. We will use embeddings from the CLIP image encoder to represent garment attributes. Similarity between items is then calculated using the cosine similarity of these embedding vectors.
•	Hybrid Approach: The final recommendations are a weighted combination of scores from both models, ensuring both personalization and the ability to recommend new items.
5.4. Module 3: Smart Search (Text + Image)
•	Methodology: All product images are first encoded into 512-dimensional vectors using the CLIP image encoder. These vectors are stored in a FAISS (Facebook AI Similarity Search) index for efficient similarity search.
o	For a text query, the text is encoded using the CLIP text encoder, and the resulting vector is used to find the most similar image vectors.
o	For an image query, the image is encoded, and its vector is used to find similar image vectors.
•	Similarity Score: We use Cosine Similarity to measure the closeness between two vectors (x,y) in the embedding space. $$ \text{sim}(x,y) = \frac{x \cdot y}{||x|| , ||y||} $$
